{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "github_response = requests.get('https://github.com/trending/developers')\n",
    "html_github = github_response.content\n",
    "github_soup = bs4.BeautifulSoup(html_github, \"html.parser\")\n",
    "#github_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Anton Babenko (antonbabenko)',\n",
       " 'Fernand Galiana (derailed)',\n",
       " 'Joe Chen (unknwon)',\n",
       " 'Alex Goodman (wagoodman)',\n",
       " 'kezhenxu94 (kezhenxu94)',\n",
       " 'Georgios Konstantopoulos (gakonst)',\n",
       " 'Zachary Rice (zricethezav)',\n",
       " 'Steve Macenski (SteveMacenski)',\n",
       " 'Asim Aslam (asim)',\n",
       " 'Evan Wallace (evanw)',\n",
       " 'Gajus Kuizinas (gajus)',\n",
       " 'Eliza Weisman (hawkw)',\n",
       " 'John Lindquist (johnlindquist)',\n",
       " 'Rijk van Zanten (rijkvanzanten)',\n",
       " 'Robert Mosolgo (rmosolgo)',\n",
       " 'Mariusz Nowak (medikoo)',\n",
       " 'Casey Davenport (caseydavenport)',\n",
       " 'Rich Harris (Rich)',\n",
       " 'R.I.Pienaar (ripienaar)',\n",
       " 'Carlos Alexandro Becker (caarlos0)',\n",
       " 'Shivam Mathur (shivammathur)',\n",
       " 'Chocobozzz (Chocobozzz)',\n",
       " 'Alex Potsides (achingbrain)',\n",
       " 'Rico Sta. Cruz (rstacruz)',\n",
       " 'Stephen Celis (stephencelis)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "tag_dev = github_soup.find_all(\"h1\", {\"class\":\"h3 lh-condensed\"})\n",
    "developers = []\n",
    "for i in range(0,len(tag_dev)):\n",
    "    tag_dev_str = str(tag_dev[i])\n",
    "    pattern = 'href=\\W\\W\\w+'\n",
    "    words = str(re.findall(pattern, tag_dev_str))\n",
    "    words = str(words.replace(\"[\\'href=\",\"\"))\n",
    "    words = str(words.replace(\"']\",\")\"))\n",
    "    words = str(words.replace('\"',\"\"))\n",
    "    words = str(words.replace(\"/\",\"(\"))\n",
    "    tag_dev_str_name = str(tag_dev[i].a.string)\n",
    "    tag_dev_str_name = tag_dev_str_name.replace(\"\\n\",\"\")\n",
    "    tag_dev_str_name = tag_dev_str_name.strip()\n",
    "    developers_str = tag_dev_str_name + \" \" + words\n",
    "    developers.append(developers_str)\n",
    "developers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fastapi',\n",
       " 'youtube-dl',\n",
       " 'log4j-tools',\n",
       " 'log4j-finder',\n",
       " 'turtle',\n",
       " 'black',\n",
       " 'prefect',\n",
       " 'minDALL-E',\n",
       " 'epicgames-claimer',\n",
       " 'saleor',\n",
       " 'poetry',\n",
       " 'PaddleClas',\n",
       " 'python-cheatsheet',\n",
       " 'pytorch3d',\n",
       " 'zulip',\n",
       " 'django-rest-framework',\n",
       " 'yt-dlp',\n",
       " 'Yuno',\n",
       " 'PaddleOCR',\n",
       " 'pycord',\n",
       " 'PaddleSpeech',\n",
       " 'moco',\n",
       " 'auto-cpufreq',\n",
       " 'segmentation_models.pytorch',\n",
       " 'ARKitScenes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "github_response_trending = requests.get('https://github.com/trending/python?since=daily')\n",
    "html_github_trending = github_response_trending.content\n",
    "github_soup_trending = bs4.BeautifulSoup(html_github_trending, \"html.parser\")\n",
    "tag_trend_rep = github_soup_trending.find_all(\"h1\", {\"class\":\"h3 lh-condensed\"})\n",
    "\n",
    "#OPTION ONE\n",
    "trending_repositories = [i.a[\"href\"].split(\"/\")[2] for i in tag_trend_rep]\n",
    "trending_repositories\n",
    "\n",
    "# ANOTHER WAY:\n",
    "#trending_repositories = []\n",
    "#for i in range(0,len(tag_trend_rep)):\n",
    "#    tag_trend_rep_str = str(tag_trend_rep[i])\n",
    "#    pattern_trend = '\\S+\\n</a> </h1>$'\n",
    "#    trending_repositories_str = str(re.findall(pattern_trend,tag_trend_rep_str))\n",
    "#    trending_repositories_str = str(trending_repositories_str.replace(\"\\\\n</a> </h1>\",\"\"))\n",
    "#    trending_repositories_str = str(trending_repositories_str.replace(\"[\",\"\"))\n",
    "#    trending_repositories_str = str(trending_repositories_str.replace(\"]\",\"\"))\n",
    "#    trending_repositories_str = str(trending_repositories_str.replace(\"'\",\"\"))\n",
    "#    trending_repositories.append(trending_repositories_str)\n",
    "#trending_repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/19px-P_vip.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/15px-Magic_Kingdom_castle.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/19px-Video-x-generic.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/21px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/21px-Flag_of_the_United_States.svg.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "walt_disney_response = requests.get('https://en.wikipedia.org/wiki/Walt_Disney')\n",
    "html_walt_disney = walt_disney_response.content\n",
    "walt_disney_soup = bs4.BeautifulSoup(html_walt_disney,\"html.parser\")\n",
    "walt_disney_images = walt_disney_soup.find_all(\"a\", {\"class\":\"image\"})\n",
    "\n",
    "# OPTION ONE\n",
    "wiki_walt_disney_images = [\"https:\"+i.img[\"src\"] for i in walt_disney_images]\n",
    "wiki_walt_disney_images\n",
    "\n",
    "#ANOTHER WAY\n",
    "#empty_list = []\n",
    "#for i in range(0,len(walt_disney_images)):\n",
    "#    pattern_images = '//upload.wikimedia.org/wikipedia/commons/thumb\\S+'\n",
    "#    walt_disney_str = str(walt_disney_images[i])\n",
    "#    walt_disney_images_str = re.findall(pattern_images, walt_disney_str)\n",
    "#    for x in walt_disney_images_str:\n",
    "#        empty_list.append(x)\n",
    "#wiki_walt_disney_images = [\"https:\"+d.replace('\"',\"\") for d in empty_list]\n",
    "#wiki_walt_disney_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Pythonidae',\n",
       " 'https://en.wikipedia.org/wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=1',\n",
       " 'https://en.wikipedia.org/wiki/Python_(programming_language)',\n",
       " 'https://en.wikipedia.org/wiki/CMU_Common_Lisp',\n",
       " 'https://en.wikipedia.org/wiki/PERQ#PERQ_3',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=2',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Aenus',\n",
       " 'https://en.wikipedia.org/wiki/Python_(painter)',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Byzantium',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Catana',\n",
       " 'https://en.wikipedia.org/wiki/Python_Anghelo',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=3',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Efteling)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=4',\n",
       " 'https://en.wikipedia.org/wiki/Python_(automobile_maker)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Ford_prototype)',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=5',\n",
       " 'https://en.wikipedia.org/wiki/Python_(missile)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(nuclear_primary)',\n",
       " 'https://en.wikipedia.org/wiki/Colt_Python',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=6',\n",
       " 'https://en.wikipedia.org/wiki/PYTHON',\n",
       " 'https://en.wikipedia.org/wiki/Python_(film)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(mythology)',\n",
       " 'https://en.wikipedia.org/wiki/Monty_Python',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Monty)_Pictures',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&amp;action=edit&amp;section=7',\n",
       " 'https://en.wikipedia.org/wiki/Cython',\n",
       " 'https://en.wikipedia.org/wiki/Pyton',\n",
       " 'https://en.wikipedia.org/wiki/Pithon',\n",
       " 'https://en.wikipedia.org/wiki/File:Disambig_gray.svg',\n",
       " 'https://en.wikipedia.org/wiki/Help:Disambiguation',\n",
       " 'https://en.wikipedia.org/wiki/Help:Category',\n",
       " 'https://en.wikipedia.org/wiki/Category:Disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:Human_name_disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:Disambiguation_pages_with_given-name-holder_lists',\n",
       " 'https://en.wikipedia.org/wiki/Category:Disambiguation_pages_with_short_descriptions',\n",
       " 'https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata',\n",
       " 'https://en.wikipedia.org/wiki/Category:All_article_disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:All_disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:Animal_common_name_disambiguation_pages']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "python_page_response = requests.get('https://en.wikipedia.org/wiki/Python')\n",
    "python_wikipg = python_page_response.content\n",
    "python_soup = bs4.BeautifulSoup(python_wikipg,\"html.parser\")\n",
    "python_wikipg_links = [link for link in python_soup.find(\"div\",id=\"bodyContent\").find_all(\"a\")]\n",
    "python_wikipg_links_list = [re.findall('href=\"(.*?)\"', str(i))[0] for i in python_wikipg_links]\n",
    "wikiog_links_list = [\"https://en.wikipedia.org\"+link for link in python_wikipg_links_list if link[0]==\"/\"]\n",
    "\n",
    "wikiog_links_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title 31 - Money and Finance ٭']\n",
      "The number of titles that have changed since its last release point is 1 .\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "uscode_response = requests.get('http://uscode.house.gov/download/download.shtml')\n",
    "uscode_content = uscode_response.content\n",
    "uscode_soup = bs4.BeautifulSoup(uscode_content,\"html.parser\")\n",
    "uscode_soup_titlechanged = uscode_soup.find_all(\"div\", {\"class\":\"usctitlechanged\"})\n",
    "titleschanged = []\n",
    "for i in range(len(uscode_soup_titlechanged)):\n",
    "    text_uscode = uscode_soup_titlechanged[i].text\n",
    "    text_uscode = text_uscode.replace(\"\\n\",\"\")\n",
    "    text_uscode = text_uscode.rstrip()\n",
    "    text_uscode = text_uscode.lstrip()\n",
    "    titleschanged.append(text_uscode)\n",
    "print (titleschanged)\n",
    "print (\"The number of titles that have changed since its last release point is\" ,len(titleschanged),\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'OCTAVIANO JUAREZ-CORRO',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'EUGENE PALMER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "fbi_response = requests.get(\"https://www.fbi.gov/wanted/topten\")\n",
    "fbi_html = fbi_response.content\n",
    "fbi_soup = bs4.BeautifulSoup(fbi_html, \"html.parser\")\n",
    "fbi_names_soup = fbi_soup.find_all(\"h3\",{\"class\":\"title\"})\n",
    "list_most_wanted_fbi = []\n",
    "for i in fbi_names_soup:\n",
    "#    i = i.text\n",
    "#    i = i.replace(\"\\n\",\"\")\n",
    "#    list_most_wanted_fbi.append(i)\n",
    "    list_most_wanted_fbi.append(i.a.string)\n",
    "list_most_wanted_fbi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time UTC</th>\n",
       "      <th>Latitude degrees</th>\n",
       "      <th>Longitude degrees</th>\n",
       "      <th>Region name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>22:40:06.0</td>\n",
       "      <td>7.51 S</td>\n",
       "      <td>121.90 E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>22:36:26.0</td>\n",
       "      <td>11.05 N</td>\n",
       "      <td>85.93 W</td>\n",
       "      <td>NICARAGUA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>22:30:37.0</td>\n",
       "      <td>8.90 N</td>\n",
       "      <td>126.28 E</td>\n",
       "      <td>MINDANAO, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>22:23:45.0</td>\n",
       "      <td>7.91 S</td>\n",
       "      <td>119.66 E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>22:16:02.0</td>\n",
       "      <td>37.88 S</td>\n",
       "      <td>74.81 W</td>\n",
       "      <td>OFF COAST OF BIO-BIO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>22:15:56.2</td>\n",
       "      <td>13.20 N</td>\n",
       "      <td>88.60 W</td>\n",
       "      <td>EL SALVADOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>22:05:10.7</td>\n",
       "      <td>19.15 N</td>\n",
       "      <td>155.50 W</td>\n",
       "      <td>ISLAND OF HAWAII, HAWAII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>22:04:49.4</td>\n",
       "      <td>59.88 N</td>\n",
       "      <td>153.88 W</td>\n",
       "      <td>SOUTHERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>22:03:30.0</td>\n",
       "      <td>7.92 S</td>\n",
       "      <td>119.74 E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:40:34.8</td>\n",
       "      <td>37.63 N</td>\n",
       "      <td>14.95 E</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:37:18.0</td>\n",
       "      <td>7.94 S</td>\n",
       "      <td>119.72 E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:34:05.0</td>\n",
       "      <td>7.94 S</td>\n",
       "      <td>119.74 E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:28:32.9</td>\n",
       "      <td>37.77 N</td>\n",
       "      <td>14.61 E</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:24:45.9</td>\n",
       "      <td>37.78 N</td>\n",
       "      <td>14.66 E</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:21:04.0</td>\n",
       "      <td>18.05 S</td>\n",
       "      <td>69.57 W</td>\n",
       "      <td>TARAPACA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:19:26.0</td>\n",
       "      <td>1.02 N</td>\n",
       "      <td>122.65 E</td>\n",
       "      <td>MINAHASA, SULAWESI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:17:41.0</td>\n",
       "      <td>0.65 S</td>\n",
       "      <td>131.53 E</td>\n",
       "      <td>NEAR N COAST OF PAPUA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:15:45.0</td>\n",
       "      <td>8.05 S</td>\n",
       "      <td>122.41 E</td>\n",
       "      <td>FLORES REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:13:07.0</td>\n",
       "      <td>7.99 S</td>\n",
       "      <td>119.70 E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-12-18</td>\n",
       "      <td>21:12:52.0</td>\n",
       "      <td>0.34 S</td>\n",
       "      <td>119.45 E</td>\n",
       "      <td>MINAHASA, SULAWESI, INDONESIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date    Time UTC Latitude degrees Longitude degrees  \\\n",
       "0   2021-12-18  22:40:06.0           7.51 S          121.90 E   \n",
       "1   2021-12-18  22:36:26.0          11.05 N           85.93 W   \n",
       "2   2021-12-18  22:30:37.0           8.90 N          126.28 E   \n",
       "3   2021-12-18  22:23:45.0           7.91 S          119.66 E   \n",
       "4   2021-12-18  22:16:02.0          37.88 S           74.81 W   \n",
       "5   2021-12-18  22:15:56.2          13.20 N           88.60 W   \n",
       "6   2021-12-18  22:05:10.7          19.15 N          155.50 W   \n",
       "7   2021-12-18  22:04:49.4          59.88 N          153.88 W   \n",
       "8   2021-12-18  22:03:30.0           7.92 S          119.74 E   \n",
       "9   2021-12-18  21:40:34.8          37.63 N           14.95 E   \n",
       "10  2021-12-18  21:37:18.0           7.94 S          119.72 E   \n",
       "11  2021-12-18  21:34:05.0           7.94 S          119.74 E   \n",
       "12  2021-12-18  21:28:32.9          37.77 N           14.61 E   \n",
       "13  2021-12-18  21:24:45.9          37.78 N           14.66 E   \n",
       "14  2021-12-18  21:21:04.0          18.05 S           69.57 W   \n",
       "15  2021-12-18  21:19:26.0           1.02 N          122.65 E   \n",
       "16  2021-12-18  21:17:41.0           0.65 S          131.53 E   \n",
       "17  2021-12-18  21:15:45.0           8.05 S          122.41 E   \n",
       "18  2021-12-18  21:13:07.0           7.99 S          119.70 E   \n",
       "19  2021-12-18  21:12:52.0           0.34 S          119.45 E   \n",
       "\n",
       "                         Region name  \n",
       "0                         FLORES SEA  \n",
       "1                          NICARAGUA  \n",
       "2              MINDANAO, PHILIPPINES  \n",
       "3                         FLORES SEA  \n",
       "4        OFF COAST OF BIO-BIO, CHILE  \n",
       "5                        EL SALVADOR  \n",
       "6           ISLAND OF HAWAII, HAWAII  \n",
       "7                    SOUTHERN ALASKA  \n",
       "8                         FLORES SEA  \n",
       "9                      SICILY, ITALY  \n",
       "10                        FLORES SEA  \n",
       "11                        FLORES SEA  \n",
       "12                     SICILY, ITALY  \n",
       "13                     SICILY, ITALY  \n",
       "14                   TARAPACA, CHILE  \n",
       "15     MINAHASA, SULAWESI, INDONESIA  \n",
       "16  NEAR N COAST OF PAPUA, INDONESIA  \n",
       "17          FLORES REGION, INDONESIA  \n",
       "18                        FLORES SEA  \n",
       "19     MINAHASA, SULAWESI, INDONESIA  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "EMSC_response = requests.get('https://www.emsc-csem.org/Earthquake/')\n",
    "EMSC_html = EMSC_response.content\n",
    "EMSC_soup = bs4.BeautifulSoup(EMSC_html, \"html.parser\")\n",
    "EMSC_earthquake_table = EMSC_soup.find_all (\"th\",{\"class\":\"th2\"})\n",
    "\n",
    "#COLUMNS - EVENTUALLY I WILL ONLY USE VALUES IN INDEXES 0,1,2 AND 5\n",
    "total_columns = [EMSC_earthquake_table[i].text.replace('  [+]','').replace(' & ','_') for i in range(1,len(EMSC_earthquake_table)-1)]\n",
    "#print (total_columns)\n",
    "\n",
    "#DATE & TIME\n",
    "EMSC_earthquake_datetime = EMSC_soup.find_all (\"td\",{\"class\":\"tabev6\"})\n",
    "date_time = [i.a.string.replace('\\xa0\\xa0\\xa0','_') for i in EMSC_earthquake_datetime[0:20]]\n",
    "#print (date_time)\n",
    "\n",
    "#LONGITUDE & LATITUDE\n",
    "EMSC_earthquake_latlong = EMSC_soup.find_all (\"td\",{\"class\":\"tabev1\"})\n",
    "latitude = [i.string.replace('\\xa0','') for i in EMSC_earthquake_latlong [0:40:2]]\n",
    "longitude = [i.string.replace('\\xa0','') for i in EMSC_earthquake_latlong[1:40:2]]\n",
    "\n",
    "EMSC_earthquake_direction = EMSC_soup.find_all (\"td\",{\"class\":\"tabev2\"})\n",
    "latitude_dir = [i.string.replace('\\xa0\\xa0','') for i in EMSC_earthquake_direction[0:60:3]]\n",
    "longitude_dir = [i.string.replace('\\xa0\\xa0','') for i in EMSC_earthquake_direction[1:60:3]]\n",
    "\n",
    "latitude_cleaned = [latitude[i] + ' ' + latitude_dir[i] for i in range(0,len(latitude))]\n",
    "longitude_cleaned = [longitude[i] + ' ' + longitude_dir[i] for i in range(0,len(longitude))]\n",
    "#print (latitude_cleaned)\n",
    "#print (longitude_cleaned)\n",
    "\n",
    "#REGION\n",
    "EMSC_earthquake_region = EMSC_soup.find_all (\"td\",{\"class\":\"tb_region\"})\n",
    "region = [i.string.replace('\\xa0','') for i in EMSC_earthquake_region[0:20]]\n",
    "#print(region)\n",
    "\n",
    "#DATAFRAME\n",
    "Earthquake_df = pd.DataFrame(date_time, columns = [total_columns[0]])\n",
    "Earthquake_df[['Date', 'Time UTC']] = Earthquake_df['Date_Time UTC'].str.split('_', expand=True)\n",
    "del Earthquake_df['Date_Time UTC']\n",
    "Earthquake_df[total_columns[1]] = latitude_cleaned\n",
    "Earthquake_df[total_columns[2]] = longitude_cleaned\n",
    "Earthquake_df[total_columns[5]] = region\n",
    "Earthquake_df\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the screen name of a user in twitter: @irene9ago\n",
      "Username not found\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "#I HAVE TRIED WITH SEVERAL WAYS BUT IT SEEMS I'M DOING SOMETHING WRONG OR SOMETHING AS IT TELLS ME EITHER THE USER DOES \n",
    "#NOT EXIST, IT TAKES TOO LONG AND I HAVE TO RESTART THE KERNEL OR I GET AN ERROR\n",
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler  \n",
    "\n",
    "api_key = \"nQfNxnXSgCHcC92s43LysF7Xb\"\n",
    "api_key_secret = \"enjV0TU0Suu28LEqcV9v4RKd0XpXPeTAGcrK4LJ4E5pBEU6FAu\"\n",
    "access_token = \"266853131-1zx3XNz4rRTWcNm4oyE7iXPlHtcNfEFXRfcvJUTk\"\n",
    "token_secret = \"et3c4YMWgQoJUvsUM9Y7yeJbdT4pVgmEYUWVWqJF6OxMW\"\n",
    "\n",
    "# Configuration\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "user_name= input(str(\"Enter the screen name of a user in twitter: \"))\n",
    "\n",
    "try:\n",
    "    username = api.get_user(user_name)\n",
    "    tweet_count = user.statuses_count \n",
    "    print(\"The number of tweets the user \" + user_name + \" has posted is : \" + str(statuses_count))\n",
    "\n",
    "except:\n",
    "     print(\"Username not found\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'snscrape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18644/1244645829.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msnscrape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtwitter\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msntwitter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtw2_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Enter a user of a Twitter account: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muser_random_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"irene9ago85\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'snscrape'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "tw2_acc = input('Enter a user of a Twitter account: ')\n",
    "\n",
    "user_random_name = \"irene9ago85\" \n",
    "\n",
    "scraped_tweets = sntwitter.TwitterSearchScraper('from:'+user_random_name).get_items()\n",
    "df_Irene9ago85 = pd.DataFrame(scraped_tweets)[['date', 'id', 'content', 'username']]\n",
    "display(df_irene9ago85)\n",
    "\n",
    "print(\"number of tweets of @\" + user_random_name, len(df_irene9ago85))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the screen name of the user in twitter: @irene9ago\n",
      "Username not found\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "user = input(str(\"Enter the screen name of the user in twitter: \"))\n",
    "try:\n",
    "    followers_count = api.get_user(screen_name=user).followers_count\n",
    "    print(\"The number of followers of the user \" + user +\" is: \" + str(followers_count))\n",
    "except:\n",
    "     print(\"Username not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English: 6.383.000+ articles',\n",
       " '日本語: 1.292.000+ 記事',\n",
       " 'Русский: 1.756.000+ статей',\n",
       " 'Deutsch: 2.617.000+ Artikel',\n",
       " 'Español: 1.717.000+ artículos',\n",
       " 'Français: 2.362.000+ articles',\n",
       " '中文: 1.231.000+ 條目',\n",
       " 'Italiano: 1.718.000+ voci',\n",
       " 'Português: 1.074.000+ artigos',\n",
       " 'Polski: 1.490.000+ haseł']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "languages_response = requests.get('https://www.wikipedia.org/')\n",
    "languages_html = languages_response.content\n",
    "languages_soup = bs4.BeautifulSoup(languages_html,\"html.parser\")\n",
    "\n",
    "#AS A LIST\n",
    "languages_soup_parsed = languages_soup.find_all(\"div\", {\"class\":\"central-featured-lang\"})\n",
    "languages_list = [i.a.strong.string + \": \" + i.a.bdi.string.replace(\"\\xa0\",\".\") + \" \" + i.a.span.string for i in languages_soup_parsed]\n",
    "#languages_list = [i.text.replace('\\xa0','.').replace('\\n',' ').rstrip().lstrip() for i in languages_soup_parsed]\n",
    "languages_list  \n",
    "\n",
    "# AS DATAFRAME\n",
    "#languages = [i.text.strip() for i in languages_soup.find_all(\"strong\")][1:-1]\n",
    "#articles = [x.text.replace(\"\\xa0\",\".\").strip() for x in languages_soup.find_all(\"small\")][0:(len(languages))]\n",
    "#languages_articles_pd = pd.DataFrame(languages, columns = [\"Language\"])\n",
    "#languages_articles_pd [\"Articles\"] = pd.DataFrame(articles)\n",
    "#languages_articles_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "datasets_response = requests.get('https://data.gov.uk/')\n",
    "datasets_html = datasets_response.content\n",
    "datasets_soup = bs4.BeautifulSoup(datasets_html,\"html.parser\")\n",
    "datasets_soup_parsed = datasets_soup.find_all(\"h3\",{\"govuk-heading-s dgu-topics__heading\"})\n",
    "datasets = [i.a.string for i in datasets_soup_parsed]\n",
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "      <th>Percentageof world pop.(March 2019)</th>\n",
       "      <th>Language family</th>\n",
       "      <th>Branch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "      <td>11.922%</td>\n",
       "      <td>Sino-Tibetan</td>\n",
       "      <td>Sinitic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "      <td>5.994%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "      <td>4.922%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Germanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hindi (sanskritised Hindustani)</td>\n",
       "      <td>341</td>\n",
       "      <td>4.429%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>300</td>\n",
       "      <td>4.000%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "      <td>2.870%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "      <td>2.000%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Balto-Slavic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "      <td>1.662%</td>\n",
       "      <td>Japonic</td>\n",
       "      <td>Japanese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Western Punjabi</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.204%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.079%</td>\n",
       "      <td>Indo-European</td>\n",
       "      <td>Indo-Aryan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank                           Language   Speakers(millions)   \\\n",
       "0    1                   Mandarin Chinese                  918    \n",
       "1    2                            Spanish                  480    \n",
       "2    3                            English                  379    \n",
       "3    4    Hindi (sanskritised Hindustani)                  341    \n",
       "4    5                            Bengali                  300    \n",
       "5    6                         Portuguese                  221    \n",
       "6    7                            Russian                  154    \n",
       "7    8                           Japanese                  128    \n",
       "8    9                    Western Punjabi                 92.7    \n",
       "9   10                            Marathi                 83.1    \n",
       "\n",
       "   Percentageof world pop.(March 2019)   Language family          Branch  \n",
       "0                              11.922%      Sino-Tibetan         Sinitic  \n",
       "1                               5.994%     Indo-European         Romance  \n",
       "2                               4.922%     Indo-European        Germanic  \n",
       "3                               4.429%     Indo-European      Indo-Aryan  \n",
       "4                               4.000%     Indo-European      Indo-Aryan  \n",
       "5                               2.870%     Indo-European         Romance  \n",
       "6                               2.000%     Indo-European    Balto-Slavic  \n",
       "7                               1.662%           Japonic        Japanese  \n",
       "8                               1.204%     Indo-European      Indo-Aryan  \n",
       "9                               1.079%     Indo-European      Indo-Aryan  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "top10languages_response = requests.get('https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers')\n",
    "top10languages_html = top10languages_response.content\n",
    "top10languages_soup = bs4.BeautifulSoup(top10languages_html,\"html.parser\")\n",
    "top10languages = top10languages_soup.find_all(\"tr\")\n",
    "table_top10languages = [i.text.replace(\"\\n\\n\",\" , \").replace(\"\\n\",\"\").replace(\"[11]\",\"\").replace(\"[10]\",\"\").replace(\"[12]\",\"\").split(',') for i in top10languages][1:12]\n",
    "columns = [table_top10languages[0]]\n",
    "top10languages_df = pd.DataFrame(table_top10languages[1:11], columns = columns)\n",
    "top10languages_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Initial Release</th>\n",
       "      <th>Director Name</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>La batalla de Argel</td>\n",
       "      <td>(1966)</td>\n",
       "      <td>Gillo Pontecorvo</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Las noches de Cabiria</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>Federico Fellini</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>La princesa prometida</td>\n",
       "      <td>(1987)</td>\n",
       "      <td>Rob Reiner</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Paris, Texas</td>\n",
       "      <td>(1984)</td>\n",
       "      <td>Wim Wenders</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Tres colores: Rojo</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>Krzysztof Kieslowski</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Movie Name Initial Release         Director Name Stars\n",
       "0          Cadena perpetua          (1994)        Frank Darabont   9.2\n",
       "1               El padrino          (1972)  Francis Ford Coppola   9.1\n",
       "2     El padrino: Parte II          (1974)  Francis Ford Coppola   9.0\n",
       "3      El caballero oscuro          (2008)     Christopher Nolan   9.0\n",
       "4    12 hombres sin piedad          (1957)          Sidney Lumet   8.9\n",
       "..                     ...             ...                   ...   ...\n",
       "245    La batalla de Argel          (1966)      Gillo Pontecorvo   8.0\n",
       "246  Las noches de Cabiria          (1957)      Federico Fellini   8.0\n",
       "247  La princesa prometida          (1987)            Rob Reiner   8.0\n",
       "248           Paris, Texas          (1984)           Wim Wenders   8.0\n",
       "249     Tres colores: Rojo          (1994)  Krzysztof Kieslowski   8.0\n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "imdb_response = requests.get('https://www.imdb.com/chart/top').content\n",
    "imdb_soup = bs4.BeautifulSoup(imdb_response,\"html.parser\")\n",
    "imdb_parser = imdb_soup.find_all(\"table\",{\"class\":\"chart full-width\"})[0].find_all(\"a\")\n",
    "imdb_top250_director = [i[\"title\"].split(\"(dir.)\")[0].strip() for i in imdb_parser if i.string !=None]\n",
    "imdb_top250_movie_name = [ i.string for i in imdb_parser[1:len(imdb_parser):2]]\n",
    "imdb_top_year = imdb_soup.find_all(\"span\",{\"class\":\"secondaryInfo\"})\n",
    "imdb_top250_year = [i.string for i in imdb_top_year]\n",
    "imdb_top_stars = imdb_soup.find_all(\"td\",{\"class\":\"ratingColumn imdbRating\"})\n",
    "imdb_top250_stars = [i.text.strip() for i in imdb_top_stars]\n",
    "imdb_top250_df = pd.DataFrame({\"Movie Name\":imdb_top250_movie_name,\"Initial Release\":imdb_top250_year,\"Director Name\":imdb_top250_director,\"Stars\":imdb_top250_stars})\n",
    "imdb_top250_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\irene\\anaconda3\\envs\\ironhack\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Initial Release</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>Two imprisoned men bond over a number of years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>(1972)</td>\n",
       "      <td>The Godfather follows Vito Corleone, Don of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>(1974)</td>\n",
       "      <td>The early life and career of Vito Corleone in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>(2008)</td>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>(1957)</td>\n",
       "      <td>The jury in a New York City murder trial is fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>La lista de Schindler</td>\n",
       "      <td>(1993)</td>\n",
       "      <td>In German-occupied Poland during World War II,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>El señor de los anillos: El retorno del rey</td>\n",
       "      <td>(2003)</td>\n",
       "      <td>Gandalf and Aragorn lead the World of Men agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Spider-Man: No Way Home</td>\n",
       "      <td>(2021)</td>\n",
       "      <td>With Spider-Man's identity now revealed, Peter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>(1994)</td>\n",
       "      <td>The lives of two mob hitmen, a boxer, a gangst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>El bueno, el feo y el malo</td>\n",
       "      <td>(1966)</td>\n",
       "      <td>A bounty hunting scam joins two men in an unea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Movie Name Initial Release  \\\n",
       "0                              Cadena perpetua          (1994)   \n",
       "1                                   El padrino          (1972)   \n",
       "2                         El padrino: Parte II          (1974)   \n",
       "3                          El caballero oscuro          (2008)   \n",
       "4                        12 hombres sin piedad          (1957)   \n",
       "5                        La lista de Schindler          (1993)   \n",
       "6  El señor de los anillos: El retorno del rey          (2003)   \n",
       "7                      Spider-Man: No Way Home          (2021)   \n",
       "8                                 Pulp Fiction          (1994)   \n",
       "9                   El bueno, el feo y el malo          (1966)   \n",
       "\n",
       "                                         Description  \n",
       "0  Two imprisoned men bond over a number of years...  \n",
       "1  The Godfather follows Vito Corleone, Don of th...  \n",
       "2  The early life and career of Vito Corleone in ...  \n",
       "3  When the menace known as the Joker wreaks havo...  \n",
       "4  The jury in a New York City murder trial is fr...  \n",
       "5  In German-occupied Poland during World War II,...  \n",
       "6  Gandalf and Aragorn lead the World of Men agai...  \n",
       "7  With Spider-Man's identity now revealed, Peter...  \n",
       "8  The lives of two mob hitmen, a boxer, a gangst...  \n",
       "9  A bounty hunting scam joins two men in an unea...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "imdb_soup = bs4.BeautifulSoup(requests.get('https://www.imdb.com/chart/top').content,\"html.parser\")\n",
    "imdb_description_link = ['http://www.imdb.com'+re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\",str(link))[0] for link in imdb_soup.find_all(\"td\", {\"class\": \"titleColumn\"})]\n",
    "imdb_top250_df[\"Link Description\"] = imdb_description_link\n",
    "\n",
    "imdb_top10_movies = imdb_top250_df[0:10]\n",
    "movie_description = []\n",
    "for link in imdb_top10_movies[\"Link Description\"]:\n",
    "    description_soup = bs4.BeautifulSoup(requests.get(link).content, \"html.parser\") \n",
    "    description_parsed = [i.span.text for i in description_soup.find_all(\"p\",{\"class\":\"GenresAndPlot__Plot-cum89p-6 bUyrda\"})]\n",
    "    movie_description.append(description_parsed[0])\n",
    "imdb_top10_movies[\"Description\"] = movie_description\n",
    "imdb_top10_table = imdb_top10_movies[[\"Movie Name\",\"Initial Release\",\"Description\"]]\n",
    "imdb_top10_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city: Madrid\n"
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IT TAKES TOO LONG AND I HAVE TO RESTART THE KERNEL SOMETHING SEEMS TO BE INCORRECT\n",
    "# your code here\n",
    "weather_response = requests.get('http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric').content\n",
    "weather_html = bs4.BeautifulSoup(weather_response,\"html.parser\")\n",
    "weather_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>£51.77</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>£53.74</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>£50.10</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>£47.82</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>£54.23</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>£22.65</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets of Getting Your Dream...</td>\n",
       "      <td>£33.34</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A Novel Based on the Life of...</td>\n",
       "      <td>£17.93</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the Boat: Nine Americans and Their...</td>\n",
       "      <td>£22.60</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>£52.15</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade Trilogy, #1)</td>\n",
       "      <td>£13.99</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>£20.66</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>£17.46</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little Life (Scott Pi...</td>\n",
       "      <td>£52.29</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and Start Again</td>\n",
       "      <td>£35.02</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be Your Life: Scenes from the A...</td>\n",
       "      <td>£57.25</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>£23.88</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science Fiction Stories 18...</td>\n",
       "      <td>£37.59</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>£51.33</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>£45.17</td>\n",
       "      <td>In stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Book name   Price Availability\n",
       "0                                A Light in the Attic  £51.77     In stock\n",
       "1                                  Tipping the Velvet  £53.74     In stock\n",
       "2                                          Soumission  £50.10     In stock\n",
       "3                                       Sharp Objects  £47.82     In stock\n",
       "4               Sapiens: A Brief History of Humankind  £54.23     In stock\n",
       "5                                     The Requiem Red  £22.65     In stock\n",
       "6   The Dirty Little Secrets of Getting Your Dream...  £33.34     In stock\n",
       "7   The Coming Woman: A Novel Based on the Life of...  £17.93     In stock\n",
       "8   The Boys in the Boat: Nine Americans and Their...  £22.60     In stock\n",
       "9                                     The Black Maria  £52.15     In stock\n",
       "10     Starving Hearts (Triangular Trade Trilogy, #1)  £13.99     In stock\n",
       "11                              Shakespeare's Sonnets  £20.66     In stock\n",
       "12                                        Set Me Free  £17.46     In stock\n",
       "13  Scott Pilgrim's Precious Little Life (Scott Pi...  £52.29     In stock\n",
       "14                          Rip it Up and Start Again  £35.02     In stock\n",
       "15  Our Band Could Be Your Life: Scenes from the A...  £57.25     In stock\n",
       "16                                               Olio  £23.88     In stock\n",
       "17  Mesaerion: The Best Science Fiction Stories 18...  £37.59     In stock\n",
       "18                       Libertarianism for Beginners  £51.33     In stock\n",
       "19                            It's Only the Himalayas  £45.17     In stock"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "books_response = requests.get('http://books.toscrape.com/')\n",
    "books_html = books_response.content\n",
    "books_soup = bs4.BeautifulSoup(books_html,\"html.parser\") \n",
    "book_name = books_soup.find_all(\"ol\",{\"class\":\"row\"})[0]\n",
    "book_name_article = book_name.find_all(\"article\")\n",
    "book_name_list = [book.h3.a[\"title\"] for book in book_name_article]\n",
    "book_price_list = [price.string for price in books_soup.find_all(\"p\",{\"class\":\"price_color\"})]\n",
    "book_stock_list = [stock.text.strip() for stock in books_soup.find_all(\"p\",{\"class\":\"instock availability\"})]\n",
    "books = pd.DataFrame({\"Book name\":book_name_list,\"Price\":book_price_list,\"Availability\":book_stock_list})\n",
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
